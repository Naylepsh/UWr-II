{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23011601\n"
     ]
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "print(file_len('polish_corpora.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_words_from_file(file_name, lines_to_read):\n",
    "    words = {}\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for _ in range(lines_to_read):\n",
    "            words_in_line = next(f).rstrip('\\n').split(' ')\n",
    "            for word in words_in_line:\n",
    "                words[word.lower()] = True\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gather_words_from_file(file_name='polish_corpora.txt', lines_to_read=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(tokenization_attempt, proper_tokenization):\n",
    "    score = 0\n",
    "    for token in tokenization_attempt:\n",
    "        if token in proper_tokenization:\n",
    "            score +=1\n",
    "    return score / len(proper_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(['t1', 't2t3', 't3'], ['t1', 't2', 't3', 't3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_match(text):\n",
    "    tokens = []\n",
    "    current_token = ''\n",
    "    for letter in text:\n",
    "        extended_token = current_token + letter\n",
    "        if extended_token in words:\n",
    "            current_token = extended_token\n",
    "        else:\n",
    "            tokens.append(current_token)\n",
    "            current_token = letter\n",
    "    if current_token != '':\n",
    "        tokens.append(current_token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_compare(line):\n",
    "    proper_tokenization = line.lower().split(' ')\n",
    "    processed_line = \"\".join(line.split()).lower()\n",
    "    attempted_tokenization = max_match(processed_line)\n",
    "    score = get_score(attempted_tokenization, proper_tokenization)\n",
    "    print(f'expected: {proper_tokenization}')\n",
    "    print(f'got: {attempted_tokenization}')\n",
    "    print(f'score: {score}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected: ['system', 'euroraty', 'chcesz', 'kupować', 'więcej', 'niż', 'gdzie', 'indziej', '?']\n",
      "got: ['syst', 'eme', 'uro', 'raty', 'ch', 'ces', 'zk', 'upo', 'wać', 'więc', 'ej', 'niż', 'gd', 'zie', 'ind', 'zie', 'j', '?']\n",
      "score: 0.2222222222222222\n",
      "\n",
      "expected: ['parlament', 'zdecydował', 'jednak', 'inaczej', 'i', 'przyjął', 'w', 'ustawie', 'z', 'dnia', '28.06.1996', 'r.', 'jednoinstancyjne', 'postępowanie', 'orzeczniczo-lekarskie', '.']\n",
      "got: ['par', 'lam', 'en', 'tz', 'dec', 'y', 'dowa', 'ł', 'jednak', 'ina', 'czej', 'ip', 'rzy', 'jął', 'wu', 'stawie', 'zd', 'nia', '28.', '06.', '1996r', '.', 'jedno', 'insta', 'nc', 'yj', 'nep', 'ost', 'ę', 'pow', 'anie', 'orz', 'ec', 'zn', 'ic', 'zo', '-', 'lek', 'ars', 'kie', '.']\n",
      "score: 0.1875\n",
      "\n",
      "expected: ['po', 'kampanii', 'wrześniowej', '1.dlek', 'raportowała', '77', 'czołgów', 'l.t.m.35', 'utraconych', '(', 'wraz', 'z', 'wozami', 'dowodzenia', ')', 'i', '52', 'uszkodzone', 'lub', 'zepsute', ',', 'lecz', 'ostatecznie', 'po', 'naprawach', 'straty', 'bezpowrotne', 'ograniczyły', 'się', 'do', '7', 'czołgów', 'i', 'w', 'lutym', '1940', 'roku', 'posiadano', '195', 'czołgów', 'na', 'stanie', '.']\n",
      "got: ['poka', 'mpa', 'nii', 'wrze', 'ś', 'nio', 'wej', '1.', 'dle', 'krap', 'orto', 'wała', '77', 'czo', 'ł', 'gó', 'wl', '.', 't.', 'm.', '35', 'ut', 'rac', 'on', 'ych', '(', 'wraz', 'zwo', 'zam', 'ido', 'wodze', 'nia', ')', 'i5', '2', 'usz', 'kod', 'zo', 'nel', 'ub', 'zep', 'su', 'te', ',', 'lecz', 'ost', 'ate', 'cz', 'nie', 'pona', 'prawa', 'ch', 'straty', 'bez', 'pow', 'rot', 'neo', 'granic', 'zy', 'ły', 'się', 'do', '7c', 'zo', 'ł', 'gó', 'wiw', 'lutym', '1940r', 'okup', 'osi', 'ada', 'no', '195', 'czo', 'ł', 'gó', 'wn', 'asta', 'nie', '.']\n",
      "score: 0.3023255813953488\n",
      "\n",
      "expected: ['w', 'rolach', 'głównych', 'wystąpili', 'jake', 'gyllenhaal', ',', 'forest', 'whitaker', 'oraz', 'rachel', 'mcadams', '.']\n",
      "got: ['wrol', 'ach', 'głów', 'nych', 'wyst', 'ą', 'pili', 'jake', 'gy', 'll', 'en', 'ha', 'al', ',', 'fore', 'stw', 'hita', 'ker', 'oraz', 'rach', 'elm', 'cada', 'ms', '.']\n",
      "score: 0.3076923076923077\n",
      "\n",
      "expected: ['zapieczone', 'z', 'pomidorami', ',', 'bazylią', 'i', 'serem', '...']\n",
      "got: ['zap', 'iec', 'zo', 'nez', 'pomi', 'dora', 'mi', ',', 'bazylią', 'ise', 'rem', '...']\n",
      "score: 0.375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('polish_corpora.txt', encoding='utf8') as f:\n",
    "    for _ in range(5):\n",
    "        line = next(f).rstrip('\\n')\n",
    "        tokenize_and_compare(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The file is kind of scuffed ngl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie ch ju ż naw et nie będ zie str a żn ic zk ą , a le n ie ch w o góle b ęd zie .\n",
      "Moze nie trzeba by bylo interweniowac a miejsce w zk przydaloby sie komus innemu .\n",
      "A to , czy jakis pies kryl warunkowo czy nie , czy zk jest fajny czy be ...\n"
     ]
    }
   ],
   "source": [
    "with open('polish_corpora.txt', encoding='utf-8') as f:\n",
    "        for _ in range(500000):\n",
    "            line = next(f).rstrip('\\n')\n",
    "            words_in_line = line.split(' ')\n",
    "            if 'zk' in words_in_line:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "\n",
    "class SentenceGenerator:\n",
    "    def __init__(self, successors):\n",
    "        self.successors = successors\n",
    "\n",
    "    def random_word(self):\n",
    "        return choice(list(self.successors.keys()))\n",
    "\n",
    "    def random_successor_of(self, word):\n",
    "        successors = self.successors[word]\n",
    "        return choice(successors)[1]\n",
    "\n",
    "    def has_successors(self, word):\n",
    "        return word in self.successors and len(self.successors[word]) > 0\n",
    "\n",
    "    def is_end_of_sentence(self, word):\n",
    "        return word in ['<EOS>', '.', '?', '!', '?!', '!?']\n",
    "\n",
    "    def generate_sentence(self):\n",
    "        word = self.random_word()\n",
    "        while not self.has_successors(word):\n",
    "            word = self.random_word()\n",
    "\n",
    "        sentence = [word]\n",
    "        while self.has_successors(word):\n",
    "            successor = self.random_successor_of(word)\n",
    "            word = self.create_next_word(word, successor)\n",
    "            sentence.append(successor)\n",
    "\n",
    "        return ' '.join(sentence) + ('.' if not self.is_end_of_sentence(sentence[-1]) else '')\n",
    "\n",
    "    def create_next_word(self, current_word, successor):\n",
    "        _, *rest = current_word.split(' ')\n",
    "        return ' '.join(rest + [successor])\n",
    "\n",
    "    def generate_text(self, sentences=1):\n",
    "        return '\\n'.join([self.generate_sentence() for _ in range(sentences)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_successor(successors, n, word, successor):\n",
    "    successor = (n, successor)\n",
    "    if word in successors:\n",
    "        successors[word].append(successor)\n",
    "    else:\n",
    "        successors[word] = [successor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_successors_from_bigrams(file_name, K=1):\n",
    "    successors = {}\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            n, word1, word2 = line.rstrip('\\n').split(' ')\n",
    "            if (int(n) >= K):\n",
    "                add_successor(successors, int(n), word1, word2)\n",
    "            line = f.readline()\n",
    "    return successors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "successors2 = gather_successors_from_bigrams('poleval_2grams.txt', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interferencji w kilka stacji nadawczych oraz zapiski historyczne oddział ratunkowy nie znajduję podstaw aby stany , dąbki . irena turska , dojazdowe i gromadzenie się rośliny posiadają m.in. wspomniane wcześniej 6 mówiący chce podać po konkwiście.\n",
      "kureń strzelców allsvenskan . ) uczestniczyło 30 500 ochotników strażaków ( ahl w porównianiu z jakiś film posiada ktoś pisze : wschodnia . 49 kobiet zamieszkujących głównie polskiej dopuszcza sie gdzie pojawiały w warszawie-okęciu . była miss dior . 125 mw ) linię pomiędzy 40. kadencji obecnie wersja wydana pozytywna ? <EOS>\n",
      "kazachstanem i wyróżnione w benasque.\n",
      "wilanowskiego . bogusław polch.\n",
      "upakowane , 343 . ponieważ zamek : linię otworzono tu dodawać . barbara bartuś pytała , stabilizując się wychylił się precyzyjnie ocenić przydatność danego przewoźnika do tajgi , mikroklin , torpedy te zniszczenia go zeznania za niemoralne jest potwierdzana przeprowadzeniem w czytelnej i stolarnia i społecznego wykluczenia wykonawcy posiadający dwie lipy ( stefan kraft i alanem.\n"
     ]
    }
   ],
   "source": [
    "sg2 = SentenceGenerator(successors2)\n",
    "print(sg2.generate_text(sentences=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_successors_from_trigrams(file_name, K=1):\n",
    "    successors = {}\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n').split(' ')\n",
    "            if (len(line) != 4): continue\n",
    "            \n",
    "            n, word1, word2, word3 = line\n",
    "            if (int(n) >= K):\n",
    "                add_successor(successors, int(n), word1, f'{word2} {word3}')\n",
    "    return successors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "successors3 = gather_successors_from_trigrams('poleval_3grams.txt', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gigli . <EOS>.\n",
      "archiwalnym i archiwach.\n",
      "neuroprzekaźników w mózgu.\n",
      "iwoniczu zdroju ,.\n",
      "dofinansowywania poszczególnych miast.\n"
     ]
    }
   ],
   "source": [
    "sg3 = SentenceGenerator(successors3)\n",
    "sentence = sg3.generate_text(sentences=5)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10000):\n",
    "    sentence = sg3.generate_sentence()\n",
    "    if len(sentence.split(' ')) > 4:\n",
    "        print(sentence)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NonUniformSentenceGenerator(SentenceGenerator):\n",
    "    def random_successor_of(self, word):\n",
    "        successors = self.successors[word]\n",
    "        total_occurences = sum(list(map(lambda s: s[0], successors)))\n",
    "        probabilites = list(map(lambda s: s[0] / total_occurences, successors))\n",
    "        successors = list(map(lambda s: s[1], successors))\n",
    "\n",
    "        return np.random.choice(successors, 1, probabilites)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zgodziłoby się nadawało im czci najświętszego imienia siostry miłosierdzia dla regionu podkarpackiego oddziału przedsiębiorstwa małe koszty remontów głównych eksporterów oraz określi . robert biedroń . bernardynów bydgoskich : kwiatostany ( ograniczanie obciążeń nie wiedzieliby , ryk , kierownictwu . xiv jan bloch , tobiasz , jadącego pociągu pasażerskiego ? <EOS>\n",
      "dwuznaczna . rafał sikora bogatka , dodatkowy , skórne o uzasadnionych przez kwaterę . prout.\n",
      "łach i kinkiety , przywiązując do cienkich . juliusza . elżbieta dziębowska , avril , australia zachodnia cześć jej powiększenia . 68 minucie po podbiciu . luke . 64 x więcej złotych każdego zwierzęcia na 35 zmiana subwencji przypadającej w hucknall.\n",
      "dublowane ? <EOS>\n",
      "cicholską . nice 1 złotoryja . posiada dla samic z nerwami i darem jest propozycji są zestawiane w getyndze i pociągi eic , różewicz , zarejestruj sie jeden więcej poziom odpowiedzialności ... brrr . 1c , dokumentalne o śp. stanisława . katarzyna maria urodziła im prawem zastawu , mbp ) 1929 zastępca dowódcy operacyjnego rodzajów zarodników w harrow school bus company in green racing jest betonowym postumencie , generic cialis online , zbierać pieniądze otrzymają akcje huty łabędy . wojciech ziemniak skrobiowy , society w rozwidleniu dróg oddanych przyjaciół hajnówki . mariana curyłę . podczas odmawiania modlitw na finałach ( 2000-2001 ) względnie jego wybudowanie stacji grodzisk , 1963 do zorganizowanych pod passchendaele.\n"
     ]
    }
   ],
   "source": [
    "nusg2 = NonUniformSentenceGenerator(successors2)\n",
    "print(nusg2.generate_text(sentences=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "murom . <EOS>.\n",
      "szczepione , lub.\n",
      "wachlarza możliwości ..\n",
      "biochemiczny . <EOS>.\n",
      "przystąpiło 8 zespołów.\n"
     ]
    }
   ],
   "source": [
    "nusg3 = NonUniformSentenceGenerator(successors3)\n",
    "print(nusg3.generate_text(sentences=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def compute_score_of_pair(word1, word2, successors):\n",
    "    if word1 not in successors:\n",
    "        return 0\n",
    "    if len(successors[word1]) == 0:\n",
    "        return 0\n",
    "\n",
    "    for score, successor in successors[word1]:\n",
    "        if word2 == successor:\n",
    "            return score\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def compute_sentence_score(sentence, successors):\n",
    "    score = 0\n",
    "    for i in range(len(sentence) - 1):\n",
    "        word1, word2 = sentence[i:i+2]\n",
    "        score += compute_score_of_pair(word1, word2, successors)\n",
    "    return score\n",
    "\n",
    "\n",
    "def create_sentences(words, successors, n=-1):\n",
    "    n = len(successors) if n == -1 else n\n",
    "    perms = list(permutations(words))\n",
    "    scored = list(\n",
    "        map(lambda p: (p, compute_sentence_score(p, successors)), perms))\n",
    "    return sorted(scored, key=lambda s: -s[1])[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper functions for creating bigrams from custom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "\n",
    "def compute_successors(file_name):\n",
    "    successors = {}\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            words = remove_punctuation(line.rstrip('\\n')).split(' ')\n",
    "            for i in range(len(words) - 1):\n",
    "                word1, word2 = list(map(lambda w: w.lower(), words[i:i+2]))\n",
    "                if word1 in successors:\n",
    "                    if word2 in successors[word1]:\n",
    "                        successors[word1][word2] += 1\n",
    "                    else:\n",
    "                        successors[word1][word2] = 1\n",
    "                else:\n",
    "                    successors[word1] = {word2: 1}\n",
    "\n",
    "    scored_successors = {}\n",
    "    for word in successors:\n",
    "        scored_successors[word] = []\n",
    "        for successor in successors[word]:\n",
    "            score = successors[word][successor]\n",
    "            scored_successors[word].append((score, successor))\n",
    "\n",
    "    return scored_successors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_successors = compute_successors('test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default successors from bigram file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('judyta', 'poślubiła', 'syna', 'ottona', 'margrabiego'), 21)\n",
      "(('judyta', 'poślubiła', 'margrabiego', 'syna', 'ottona'), 21)\n",
      "(('judyta', 'syna', 'ottona', 'poślubiła', 'margrabiego'), 21)\n",
      "(('judyta', 'syna', 'ottona', 'margrabiego', 'poślubiła'), 21)\n",
      "(('judyta', 'margrabiego', 'poślubiła', 'syna', 'ottona'), 21)\n",
      "(('judyta', 'margrabiego', 'syna', 'ottona', 'poślubiła'), 21)\n",
      "(('poślubiła', 'judyta', 'syna', 'ottona', 'margrabiego'), 21)\n",
      "(('poślubiła', 'judyta', 'margrabiego', 'syna', 'ottona'), 21)\n",
      "(('poślubiła', 'syna', 'ottona', 'judyta', 'margrabiego'), 21)\n",
      "(('poślubiła', 'syna', 'ottona', 'margrabiego', 'judyta'), 21)\n"
     ]
    }
   ],
   "source": [
    "words = ['judyta', 'poślubiła', 'ottona', 'syna', 'margrabiego']\n",
    "for sentence in create_sentences(words, successors, 10):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('znana', 'data', 'śmierci', 'nie', 'jest', 'również', 'Judyty'), 253408)\n",
      "(('Judyty', 'znana', 'data', 'śmierci', 'nie', 'jest', 'również'), 253408)\n",
      "(('data', 'śmierci', 'nie', 'jest', 'również', 'znana', 'Judyty'), 253359)\n",
      "(('Judyty', 'data', 'śmierci', 'nie', 'jest', 'również', 'znana'), 253359)\n",
      "(('znana', 'Judyty', 'data', 'śmierci', 'nie', 'jest', 'również'), 253285)\n",
      "(('data', 'śmierci', 'nie', 'jest', 'również', 'Judyty', 'znana'), 253285)\n",
      "(('śmierci', 'nie', 'jest', 'również', 'znana', 'data', 'Judyty'), 253252)\n",
      "(('Judyty', 'śmierci', 'nie', 'jest', 'również', 'znana', 'data'), 253252)\n",
      "(('znana', 'data', 'Judyty', 'śmierci', 'nie', 'jest', 'również'), 253178)\n",
      "(('śmierci', 'nie', 'jest', 'również', 'Judyty', 'znana', 'data'), 253178)\n"
     ]
    }
   ],
   "source": [
    "words = ['nie', 'jest', 'również', 'znana', 'data', 'śmierci', 'Judyty']\n",
    "for sentence in create_sentences(words, successors, 10):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
